\chapter{Conclusion and Future Work}
\label{chap:conclusion}

The work in this thesis gave detail insights in the key factors driving the performance of a web application. The web, it's data and complexity has grown immensely and web engineers nowadays have to have a broad horizon of what's going on in the network as well as in the browser. Network Latency is the most problematic unit when sending requests and responses over HTTP since it scales with the distance and intermediate stations on the way. Carefully thinking about where to place web content plays a key role here since transfer speed has a hard limit with the speed of light and refractions of the medium. The measurements confirmed the presumption that the network is highly dependent on other participants, routers or current load and therefore often unreliable. A bunch of concurrent and sequential requests result in high standard deviation and outliers since request rate of the web server can't grow that high to handle all requests equally. The Apache Benchmark command line tool with it's simple extensibility via shell scripts was used to execute load tests as well as consistency of requests over different time periods. Measurements showed that load also varies whether done in the night or during the day. How important it is to understand DOM processing, CSS trees and the Javascript Engine was illustrated when measuring processing in the browser with the webpagetest.org tool from Google. The waterfall diagrams showed the importance of the start render time and outlined problems regarding blocking Javascript files and URL redirects. At last a few improvements to eliminate existing problems were suggested and illustrated with short code snippets to give an idea how existing problems in the system can be solved or alleviated.

Due to the reason that Web Performance has become a hot topic, research and development is evolving continuously and there are still enough possibilities to save tens of milliseconds from the start of a request until it's rendering in the browser. Especially HTTP and it's underlying TCP connection still offers scope for improvements. HTTP/2 was approved in 2015 and sounds promising to give major advantages in reducing latency. Although it's core concept remained the same, it handles data differently under the hood. The usage of TCP connections gets more efficiently with a completely new binary framing layer which allows multiplexing of requests and responses over a single connection. With stream prioritization and server push new cool features wait to be leveraged in web applications and open opportunities which did not exist before \cite{Grigorik_2015}.


